<!doctype html><html class="no-js" lang="en" dir="ltr"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="theme-color" content="#15a"><title>Kubernetes Is Not a Platform Strategy
— Daily DevOps & .NET</title><meta name="description" content="Kubernetes has become an assumed default in many organizations, positioned as a universal platform that absorbs governance, security, observability, and …"><meta name="twitter:description" property="og:description" content="Kubernetes has become an assumed default in many organizations, positioned as a universal platform that absorbs governance, security, observability, and …"><meta name="author" content="Martin Stühmer"><link rel="icon" href="/logo.svg?v=b9e294dc5c6fa04439c8c803d8e981a9" sizes="any" type="image/svg+xml"><meta name="twitter:title" property="og:title" content="Kubernetes Is Not a Platform Strategy
 — Daily DevOps & .NET"><meta property="og:updated_time" content="2026-01-13T17:06:05+01:00"><meta property="article:modified_time" content="2026-01-13T17:06:05+01:00"><meta property="article:published_time" content="2026-01-13T17:00:00+01:00"><meta property="og:type" content="article"><meta property="og:site_name" content="Daily DevOps & .NET"><meta property="og:url" content="https://daily-devops.net/posts/kubernetes-not-platform-strategy/"><meta property="og:locale" content="en_US"><meta name="twitter:card" content="summary_large_image"><meta property="og:image" name="twitter:image" content="https://daily-devops.net/images/kubernetes-dotnet-1200x630.webp?v=60744ff51019106f1611c64bed6d8275"><meta property="og:image:secure_url" content="https://daily-devops.net/images/kubernetes-dotnet-1200x630.webp?v=60744ff51019106f1611c64bed6d8275"><meta property="og:image:alt" name="twitter:image:alt" content="Kubernetes Is Not a Platform Strategy
"><meta property="og:image:type" content="image/webp"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="og:image" name="twitter:image" content="https://daily-devops.net/images/kubernetes-dotnet-1200x630.png?v=72ed91574dfde4c7031876c5a8b5167e"><meta property="og:image:secure_url" content="https://daily-devops.net/images/kubernetes-dotnet-1200x630.png?v=72ed91574dfde4c7031876c5a8b5167e"><meta property="og:image:alt" name="twitter:image:alt" content="Kubernetes Is Not a Platform Strategy
"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><link rel="dns-prefetch" href="https://www.clarity.ms"><link rel="preconnect" href="https://www.clarity.ms" crossorigin><link rel="stylesheet" href="/css/style.min.0b2ef4378cd908eb695017aaa74603d244ff2b38c8068b4b51d76cad830c8ee5e3a40b67ab5b860a542c8bae73d8ba90cfbe9249b59c733a50de74cc32af818a.css" integrity="sha512-Cy70N4zZCOtpUBeqp0YD0kT/KzjIBotLUddsrYMMjuXjpAtnq1uGClQsi65z2LqQz76SSbWcczpQ3nTMMq+Big==" crossorigin="anonymous"><link rel="alternate" href='https://daily-devops.net/posts/kubernetes-not-platform-strategy/' hreflang="x-default" title="Kubernetes Is Not a Platform Strategy
 &mdash; Daily DevOps & .NET"><link rel="canonical" href="https://daily-devops.net/posts/kubernetes-not-platform-strategy/" hreflang="en-us" title="Kubernetes Is Not a Platform Strategy
 &mdash; Daily DevOps & .NET"><link rel="home" href="https://daily-devops.net/" hreflang="en-us" title="Daily DevOps & .NET"><link rel="next" href="https://daily-devops.net/posts/feedback-loop-ai-cant-replace/" hreflang="en-us" title="The Feedback Loop That AI Can't Replace
"><link rel="prev" href="https://daily-devops.net/posts/kehrwoche-technical-debt/" hreflang="en-us" title="Kehrwoche: What Swabian Cleaning Teaches About Technical Debt"><link rel="author" href="https://daily-devops.net/authors/martin/" hreflang="en-us" title="Martin Stühmer &mdash; Daily DevOps & .NET"><meta name="giscus:backlink" content="https://daily-devops.net/posts/kubernetes-not-platform-strategy/"><script defer src="/js/above.min.cb42b2c9a453e49e278326637d72d2f01e7f32e3f91e9cdde8fb587793d05514efd2f5cc5021c9658ca0f73876dbdcd1f7cb4a37c0bebc9df01110926b6d7cba.js" integrity="sha512-y0KyyaRT5J4ngyZjfXLS8B5/MuP5Hpzd6PtYd5PQVRTv0vXMUCHJZYyg9zh229zR98tKN8C+vJ3wERCSa218ug==" crossorigin="anonymous"></script><script type="text/javascript">(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","o0pr4n9gg3")</script></head><body class="container"><header class="header"><a class="logo" href="/" title="Daily DevOps & .NET" rel="home"><figure class="page-logo"><picture><img src="/logo.svg?v=b9e294dc5c6fa04439c8c803d8e981a9" alt="Daily DevOps & .NET" title="Daily DevOps & .NET"></picture></figure><span class="title">Daily DevOps & .NET</span></a><nav class="menu" aria-label="Main navigation"><button class="burger" type="button" aria-haspopup="menu" aria-expanded="false" aria-label="Toggle menu">
<i class="fas fa-burger" aria-hidden="true"></i></button><ul class="navigation"><li class="active"><a href="/posts/">Posts</a></li><li><a href="/authors/">About us</a></li></ul></nav><div class="divider"></div></header><main class="body"><figure class="hero"><picture><source srcset="/images/kubernetes-dotnet-544x136.webp?v=aca22bc9b1536d50ed2d50584488f163" type="image/webp" media="(max-width: 575.98px)"><source srcset="/images/kubernetes-dotnet-544x136.png?v=110a9da15aaca559ce42a145b7ba656a" type="image/png" media="(max-width: 575.98px)"><source srcset="/images/kubernetes-dotnet-672x168.webp?v=c3b34145e4a3b5cd13cd7f18fbcd8e88" type="image/webp" media="(max-width: 767.98px)"><source srcset="/images/kubernetes-dotnet-672x168.png?v=28386e5b976c691a3e2f7876b26bb7ab" type="image/png" media="(max-width: 767.98px)"><source srcset="/images/kubernetes-dotnet-896x224.webp?v=f5f5cad1fab5f21822674d4685ef4f8d" type="image/webp" media="(max-width: 991.98px)"><source srcset="/images/kubernetes-dotnet-896x224.png?v=b256ce72dc6570462ff64fbc3d3c1e8a" type="image/png" media="(max-width: 991.98px)"><source srcset="/images/kubernetes-dotnet-1104x276.webp?v=a919d662bf3bf3f12f22a15c6e1ecfb2" type="image/webp" media="(max-width: 1199.98px)"><source srcset="/images/kubernetes-dotnet-1104x276.png?v=4538b1853e0879580514fd06ab57aaa8" type="image/png" media="(max-width: 1199.98px)"><source srcset="/images/kubernetes-dotnet-1444x361.webp?v=1f5a4678ec099c3af68e57387844f008" type="image/webp"><source srcset="/images/kubernetes-dotnet-1444x361.png?v=2c1e7df6c2f98fe02b306c4436abca0b" type="image/png"><img src="/images/kubernetes-dotnet.png?v=b153621c9ec4d670255b761ea2954ae8" alt="Kubernetes Is Not a Platform Strategy
" loading="lazy" decoding="async" title="Kubernetes Is Not a Platform Strategy
"></picture></figure><article class="post"><header><h1>Kubernetes Is Not a Platform Strategy</h1></header><section class="content" role="region"><p>Kubernetes has transitioned from a technical option to an assumed default. In organizations and projects I&rsquo;ve worked with, discussions no longer start with whether Kubernetes is appropriate. They start with migration timelines. I&rsquo;ve sat through planning sessions where the question wasn&rsquo;t &ldquo;Should we use Kubernetes?&rdquo; but rather &ldquo;When can we have everything moved over?&rdquo;</p><p>This shift isn&rsquo;t driven by application requirements. It&rsquo;s driven by narrative. Consulting decks and reference architectures present <em><strong>Kubernetes as a universal platform</strong></em> that absorbs governance, security, scalability, observability, recovery, and operational responsibility. The implicit promise: once your software runs on Kubernetes, the hard parts are handled. I&rsquo;ve watched teams adopt this belief wholesale, only to discover the gaps six months into production.</p><p>That promise is incomplete. Kubernetes primarily addresses <strong>one phase</strong>: runtime orchestration. Most architectural risk, cost overruns, and operational failures occur <strong>before</strong> runtime during design and delivery, or <strong>after</strong> runtime when incidents happen and systems evolve. I&rsquo;ve debugged production incidents where Kubernetes ran flawlessly while the system failed spectacularly because architectural problems existed upstream and downstream of container orchestration.</p><p>Treating Kubernetes as a lifecycle platform rather than a runtime component introduces complexity that stays invisible during planning and becomes unavoidable in production. The demos look clean. The reference architectures are elegant. Then you hit reality.</p><p>Two questions matter: Not whether Kubernetes works (it does, consistently, in its domain), but where its responsibility ends and whether your organization can handle what lies beyond those boundaries.</p><h2 id="kubernetes-in-the-net-reality"><a href="/posts/kubernetes-not-platform-strategy/#kubernetes-in-the-net-reality" title="Kubernetes in the .NET Reality">Kubernetes in the .NET Reality</a></h2><p>Kubernetes clusters rarely host a single, clean workload type in practice. They become convergence points: ASP.NET Core APIs, background workers, event-driven processors, migrated Windows Services, and platform components all sharing infrastructure. I&rsquo;ve inherited clusters running everything from modern microservices to decade-old .NET Framework services wrapped in Windows containers, all competing for the same resources.</p><p>For stateless, Linux-based ASP.NET Core services, Kubernetes is genuinely strong. Deployments are predictable. Rollouts are controlled. Health checks integrate cleanly. You implement a simple health endpoint:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-csharp" data-lang="csharp"><span class="line"><span class="cl"><span class="kt">var</span> <span class="n">builder</span> <span class="p">=</span> <span class="n">WebApplication</span><span class="p">.</span><span class="n">CreateBuilder</span><span class="p">(</span><span class="n">args</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">builder</span><span class="p">.</span><span class="n">Services</span><span class="p">.</span><span class="n">AddHealthChecks</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="kt">var</span> <span class="n">app</span> <span class="p">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">Build</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="n">app</span><span class="p">.</span><span class="n">MapHealthChecks</span><span class="p">(</span><span class="s">&#34;/health&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">app</span><span class="p">.</span><span class="n">Run</span><span class="p">();</span>
</span></span></code></pre></div><p>Then you deploy 3 replicas and Kubernetes does what you asked: it keeps exactly 3 running, rolling out updates without downtime, removing failed pods from traffic automatically. You push a new image and watch the update complete—no manual intervention, no traffic loss, no coordination overhead.</p><p>This is where Kubernetes works exactly as intended: the application exposes its state honestly, and the platform responds intelligently. Three replicas means three replicas, constantly. A pod fails, it gets replaced within seconds. A rolling update happens seamlessly because Kubernetes orchestrates the transition and the application cooperates through its health endpoint. The first time you watch this happen without manually managing anything, it feels like magic.</p><p>This experience—predictable, reliable, hands-off—becomes the template in your mind for how Kubernetes should work everywhere.</p><p>The mistake begins when this success gets generalized. I&rsquo;ve seen this pattern repeatedly: success with stateless APIs leads to confidence that everything belongs in Kubernetes. Then the complexity arrives.</p><h2 id="governance-structure-without-enforcement"><a href="/posts/kubernetes-not-platform-strategy/#governance-structure-without-enforcement" title="Governance: Structure Without Enforcement">Governance: Structure Without Enforcement</a></h2><p>Kubernetes offers namespaces, labels, and RBAC. These are primitives, not governance. Real enterprise governance requires enforceable policy, auditability, cost attribution, and environmental separation. In Azure-centric environments, these concerns traditionally live at the subscription, management group, and Azure Policy layer, where they&rsquo;re auditable, mandatory, and enforced at the platform level.</p><p>Introducing Kubernetes adds a second governance plane. Without deliberate policy enforcement, clusters drift. I&rsquo;ve seen production and experimental workloads coexist in the same cluster because namespace isolation felt sufficient. It wasn&rsquo;t. Cost attribution becomes opaque. Who actually paid for that node pool? Which business unit owns this? When incidents happen, these questions waste critical time.</p><p>In one organization, we discovered experimental ML workloads running on production infrastructure because someone had <code>kubectl</code> access and &ldquo;just needed to test something quickly.&rdquo; The namespace separation existed. The policy enforcement didn&rsquo;t.</p><p>Kubernetes doesn&rsquo;t prevent this drift. It accelerates it by making deployment so frictionless that governance becomes an afterthought.</p><h2 id="identity-kubernetes-stops-where-entra-id-starts"><a href="/posts/kubernetes-not-platform-strategy/#identity-kubernetes-stops-where-entra-id-starts" title="Identity: Kubernetes Stops Where Entra ID Starts">Identity: Kubernetes Stops Where Entra ID Starts</a></h2><p>.NET applications rely on Entra ID (formerly Azure AD) for authentication, authorization, managed identities, and conditional access. Kubernetes has no native concept of enterprise identity. It doesn&rsquo;t integrate with Entra ID&rsquo;s policy layer, conditional access rules, or compliance tracking. This isn&rsquo;t a limitation; it&rsquo;s architectural reality.</p><p>Kubernetes RBAC governs access to cluster resources: who can deploy pods, create services, read secrets. But application identity—the identity your code runs under, the services it authenticates to, the permissions it holds—that&rsquo;s entirely separate. Kubernetes facilitates the technical handshake (workload identity token exchange), but the authority making identity decisions lives outside the cluster in Entra ID. Your application integrates with Entra ID directly, not through Kubernetes.</p><p>This boundary is invisible until you&rsquo;re three months into production and security asks about conditional access policies, device compliance rules, or audit trails. Kubernetes doesn&rsquo;t track any of that. It can&rsquo;t. The identity system is external, and Kubernetes merely provides the plumbing to connect to it.</p><p>I&rsquo;ve worked with teams who expected Kubernetes to handle enterprise identity because it handled everything else. It doesn&rsquo;t. That realization typically arrives when security reviews surface the integration gaps.</p><h2 id="networking-where-kubernetes-abstraction-fails-first"><a href="/posts/kubernetes-not-platform-strategy/#networking-where-kubernetes-abstraction-fails-first" title="Networking: Where Kubernetes Abstraction Fails First">Networking: Where Kubernetes Abstraction Fails First</a></h2><p>Networking is where Kubernetes myths collapse fastest. I&rsquo;ve seen the most preventable production incidents here. Kubernetes introduces its own networking model, but it doesn&rsquo;t replace enterprise networking. It operates <strong>inside</strong> it. This distinction matters when things go wrong.</p><p>In Azure-based architectures, your first line of defense exists outside the cluster:</p><ul><li>Virtual networks and subnet isolation</li><li>User-defined routing (UDR)</li><li>Azure Firewall or Network Virtual Appliance (NVA)</li><li>Application Gateway or Front Door with Web Application Firewall (WAF)</li><li>Private endpoints and service endpoints</li></ul><p>Ingress controllers route traffic. They don&rsquo;t defend the network. They&rsquo;re application-layer components running inside pods, not hardened network appliances.</p><p>Treating Kubernetes ingress as your security perimeter shifts responsibility from hardened network controls to application-level components that were never designed to absorb hostile traffic at scale. I&rsquo;ve seen this assumption lead to security incidents where attackers bypassed ingress controllers by targeting services directly once they gained cluster access.</p><h3 id="azure-cni-and-ip-exhaustion"><a href="/posts/kubernetes-not-platform-strategy/#azure-cni-and-ip-exhaustion" title="Azure CNI and IP Exhaustion">Azure CNI and IP Exhaustion</a></h3><p>With Azure CNI, every pod consumes a real IP address from your virtual network subnet. Scaling pods means scaling IP consumption linearly. Poor subnet sizing surfaces late—usually in production when teams suddenly can&rsquo;t scale further and the error message is cryptic. Kubernetes schedules pods until the network says no, then fails silently.</p><p>This isn&rsquo;t a Kubernetes failure. It&rsquo;s a networking responsibility that Kubernetes exposes. I&rsquo;ve debugged this scenario more times than I&rsquo;d like to admit, always with the same root cause: network planning happened before anyone calculated peak pod counts under load.</p><h3 id="east-west-traffic-and-lateral-movement"><a href="/posts/kubernetes-not-platform-strategy/#east-west-traffic-and-lateral-movement" title="East-West Traffic and Lateral Movement">East-West Traffic and Lateral Movement</a></h3><p>Kubernetes networking is flat by default. Every pod can reach every other pod within the cluster. Network policies are optional and frequently incomplete. In organizations without dedicated platform teams, they&rsquo;re often absent entirely.</p><p>For multi-service .NET systems, this makes lateral movement trivial once any single pod is compromised. An attacker who gains access to a frontend pod can immediately probe backend services, database connections, and internal APIs. Kubernetes provides the mechanism (network policies) but doesn&rsquo;t enforce discipline. I worked on an incident response where a compromised pod accessed 12 different internal services before we detected it. Network policies existed in the repository. They weren&rsquo;t applied.</p><h3 id="egress-control"><a href="/posts/kubernetes-not-platform-strategy/#egress-control" title="Egress Control">Egress Control</a></h3><p>Ingress gets constant attention: WAF rules, TLS certificates, rate limiting. Egress almost never does. By default, all pods can reach the internet: any destination, any port. In regulated environments, that&rsquo;s unacceptable. Egress control requires forced routing through Azure Firewall and explicit allow-listing of destinations.</p><p>Kubernetes has no native concept of allowed destinations. You build this external to the cluster, then spend weeks troubleshooting why perfectly valid application calls fail because someone forgot to allow-list a critical API endpoint.</p><h2 id="security-responsibility-is-concentrated-not-removed"><a href="/posts/kubernetes-not-platform-strategy/#security-responsibility-is-concentrated-not-removed" title="Security: Responsibility Is Concentrated, Not Removed">Security: Responsibility Is Concentrated, Not Removed</a></h2><p>Kubernetes provides security mechanisms. Almost none are enabled by default. A .NET application on Azure App Service benefits from opinionated defaults: automatic image scanning, encrypted secrets, preconfigured network isolation, integrated runtime monitoring.</p><p>In Kubernetes, every guarantee requires deliberate recreation:</p><ul><li>Image provenance through admission controllers and policy enforcement</li><li>Secret handling through external secret stores (Azure Key Vault integration)</li><li>Network segmentation through network policies and firewall rules</li><li>Runtime monitoring through service mesh sidecars or host-level agents</li></ul><p>Each added controller or sidecar increases capability and attack surface simultaneously. I&rsquo;ve reviewed Kubernetes configurations where security controls outnumbered application pods. The cluster became a security platform that happened to run some software.</p><p>Kubernetes doesn&rsquo;t reduce security effort. It concentrates it into your platform team, assuming you have one.</p><h2 id="cicd-and-supply-chain-kubernetes-consumes-trust"><a href="/posts/kubernetes-not-platform-strategy/#cicd-and-supply-chain-kubernetes-consumes-trust" title="CI/CD and Supply Chain: Kubernetes Consumes Trust">CI/CD and Supply Chain: Kubernetes Consumes Trust</a></h2><p>Kubernetes consumes artifacts. It doesn&rsquo;t produce trust. CI pipelines, artifact promotion, image immutability, and signing decisions all happen long before Kubernetes schedules a pod. A broken supply chain can&rsquo;t be repaired at runtime. If a malicious image makes it to your registry, Kubernetes will happily deploy it.</p><p>I&rsquo;ve worked with a team who discovered their CI pipeline had been compromised for three weeks. Kubernetes deployed every malicious image perfectly—on schedule, with zero-downtime rolling updates. The orchestration worked flawlessly. The supply chain didn&rsquo;t. Kubernetes enforces desired state but doesn&rsquo;t validate how that state was produced. That validation is your responsibility in your build pipelines, artifact registries, and admission controllers.</p><h2 id="observability-infrastructure-metrics-are-not-insight"><a href="/posts/kubernetes-not-platform-strategy/#observability-infrastructure-metrics-are-not-insight" title="Observability: Infrastructure Metrics Are Not Insight">Observability: Infrastructure Metrics Are Not Insight</a></h2><p>Kubernetes emits metrics and logs: CPU usage per pod, memory consumption, network I/O. These describe platform health, not system behavior. .NET systems require application-level observability—distributed tracing across service boundaries, dependency tracking to external systems, structured logging with correlation IDs.</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-csharp" data-lang="csharp"><span class="line"><span class="cl"><span class="n">builder</span><span class="p">.</span><span class="n">Services</span><span class="p">.</span><span class="n">AddOpenTelemetry</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="p">.</span><span class="n">WithTracing</span><span class="p">(</span><span class="n">t</span> <span class="p">=&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="n">t</span><span class="p">.</span><span class="n">AddAspNetCoreInstrumentation</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">         <span class="p">.</span><span class="n">AddHttpClientInstrumentation</span><span class="p">());</span>
</span></span></code></pre></div><p>Without integration into Azure Monitor and Application Insights, incidents become reconstruction exercises. I&rsquo;ve sat in war rooms where Kubernetes dashboards stayed green—all pods healthy, all nodes operational—while users experienced cascading timeouts. Pod restarts hide underlying failures instead of surfacing them. A pod that crashes and restarts every 30 seconds looks &ldquo;healthy&rdquo; to Kubernetes if it passes health checks between crashes.</p><p>Observability requires design. You bring it, or you debug blind.</p><h2 id="scalability-kubernetes-scales-pods-not-systems"><a href="/posts/kubernetes-not-platform-strategy/#scalability-kubernetes-scales-pods-not-systems" title="Scalability: Kubernetes Scales Pods, Not Systems">Scalability: Kubernetes Scales Pods, Not Systems</a></h2><p>Kubernetes scales replicas, not architectures. Database contention, synchronous dependencies, external API limits—they all remain regardless of how many pod copies you create. Kubernetes can amplify bottlenecks just as effectively as it amplifies capacity.</p><p>I&rsquo;ve watched auto-scaling create 50 pod replicas, all waiting for the same database connection pool that maxed out at 100 connections. More pods didn&rsquo;t solve the problem—they made it worse by consuming resources while waiting.</p><p>Event-driven scaling improves this, but only with architectural redesign. Kubernetes enables the <strong>mechanism</strong> for elasticity—you can scale replicas based on external signals. But the architecture determines whether that mechanism translates into actual scalability. Scaling 50 pods won&rsquo;t help if they&rsquo;re all waiting on the same bottleneck. That&rsquo;s a design problem, not an orchestration problem.</p><h2 id="backup-and-recovery-kubernetes-stops-completely"><a href="/posts/kubernetes-not-platform-strategy/#backup-and-recovery-kubernetes-stops-completely" title="Backup and Recovery: Kubernetes Stops Completely">Backup and Recovery: Kubernetes Stops Completely</a></h2><p>Kubernetes restarts containers. It doesn&rsquo;t restore systems. State lives outside the cluster in databases, message queues, caches, and storage accounts. Backup and recovery remain responsibilities of data platforms and operational processes. Kubernetes has no concept of business continuity or disaster recovery beyond &ldquo;restart the pod.&rdquo;</p><p>High availability masks failure. It doesn&rsquo;t undo it. A corrupted database doesn&rsquo;t care how many pod replicas exist or how fast Kubernetes can reschedule them. I&rsquo;ve responded to incidents where Kubernetes performed perfectly—immediate failover, health-driven routing—while the underlying data corruption spread across all replicas.</p><h2 id="windows-containers-on-kubernetes-a-strong-architectural-smell"><a href="/posts/kubernetes-not-platform-strategy/#windows-containers-on-kubernetes-a-strong-architectural-smell" title="Windows Containers on Kubernetes: A Strong Architectural Smell">Windows Containers on Kubernetes: A Strong Architectural Smell</a></h2><p>Windows containers are supported but introduce slower startup times (minutes versus seconds), limited ecosystem support, and operational asymmetry—separate node pools, different update cadence, higher costs. They&rsquo;re frequently used to avoid refactoring legacy workloads, turning Kubernetes into a compatibility layer rather than a platform.</p><p>I&rsquo;ve seen .NET Framework applications from 2010 wrapped in Windows containers and deployed to Kubernetes because &ldquo;we&rsquo;re moving to cloud-native.&rdquo; The workload hadn&rsquo;t changed. The infrastructure complexity increased dramatically. They function, they complicate operations, and they rarely age well.</p><p>Every Windows container deployment I&rsquo;ve reviewed eventually became a maintenance burden. The startup time alone makes scaling problematic. Windows licensing costs amplify infrastructure expenses. And the operational split between Linux and Windows node pools fragments your platform team&rsquo;s expertise.</p><h2 id="cost-and-organizational-economics"><a href="/posts/kubernetes-not-platform-strategy/#cost-and-organizational-economics" title="Cost and Organizational Economics">Cost and Organizational Economics</a></h2><p>Kubernetes isn&rsquo;t cost-neutral—a realization that typically arrives 3-6 months after initial deployment when finance asks why cloud costs doubled. It shifts cost visibility from infrastructure to organization: platform teams grow from 2 to 8 people, node pools sit idle waiting for burst capacity that happens twice a month, Windows nodes amplify costs through licensing and compute, observability instrumentation adds runtime overhead and egress costs.</p><p>Technical efficiency—improved resource utilization through bin-packing and scheduling—often comes at <strong>organizational expense</strong>: larger platform teams, slower iteration velocity (every change needs cluster-wide validation), distributed debugging complexity (which of the 15 services in the trace actually caused the timeout?).</p><p>The calculation isn&rsquo;t universal. It depends on workload mix, team structure, organizational tolerance for operational complexity. For companies running 200+ microservices with dedicated SRE teams, Kubernetes pays dividends. For companies running 8 services with 3 developers, it&rsquo;s often overhead.</p><h2 id="conclusion-kubernetes-concentrates-architectural-responsibility"><a href="/posts/kubernetes-not-platform-strategy/#conclusion-kubernetes-concentrates-architectural-responsibility" title="Conclusion: Kubernetes Concentrates Architectural Responsibility">Conclusion: Kubernetes Concentrates Architectural Responsibility</a></h2><p>Kubernetes is powerful and, in specific scenarios, the right choice: stateless Linux-based APIs with clean 12-factor design, event-driven background workers that scale horizontally, organizations with dedicated platform teams who can absorb operational complexity, and standardized workload portfolios where 80%+ of applications fit predictable patterns.</p><p>Outside these boundaries, Kubernetes doesn&rsquo;t remove responsibility. It concentrates it. The responsibilities I&rsquo;ve outlined (governance, identity, networking, security, observability, backup) don&rsquo;t disappear. They become explicit architectural decisions that someone on your team must own, implement, and maintain.</p><p>Kubernetes is not governance. That lives at the subscription, policy, and organizational level. It&rsquo;s not identity. That authority is Entra ID. It&rsquo;s not the security perimeter. That&rsquo;s the network, the firewall, and the defense-in-depth controls you build around the cluster. It&rsquo;s not backup and recovery. That responsibility belongs to data platforms and business continuity planning. It&rsquo;s not observability. That&rsquo;s an application design concern requiring deliberate instrumentation.</p><p>Kubernetes orchestrates workloads, and it does this extremely well.</p><p>From an architect&rsquo;s perspective—someone who has designed, deployed, and maintained these systems in production—Kubernetes can be the most visible component of a hosting solution but never the <strong>whole</strong> solution. The promise that it absorbs the software lifecycle is marketing, not engineering reality.</p><p>That distinction isn&rsquo;t theoretical. It&rsquo;s operational reality I&rsquo;ve experienced across multiple organizations, multiple industries, multiple failure modes.</p><p>The question isn&rsquo;t whether Kubernetes works—it does, consistently, predictably, within its domain. The question is whether your organization can handle everything Kubernetes <strong>doesn&rsquo;t</strong> do, and whether the complexity trade-off makes sense for your specific context, team capability, and workload characteristics.</p><p>Answer that question honestly before committing your platform strategy.</p></section><section class="giscus"><h2>Comments</h2><aside class="giscus" id="giscus-thread" role="region" aria-label="Comments"></aside></section><script src="https://giscus.app/client.js" data-repo="dailydevops/dailydevops.github.io" data-repo-id="R_kgDOHA8eFg" data-category="Announcements" data-category-id="DIC_kwDOHA8eFs4COIqL" data-mapping="pathname" data-strict="1" data-reactions-enabled="0" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous" async></script></article><aside class="sidebar divider"><section class="sidecar info" aria-labelledby="info-heading"><header><h2 id="info-heading" class="sr-only">Informations</h2></header><ul role="list"><li role="listitem"><time datetime="2026-01-13T17:00:00+01:00" itemprop="datePublished">Published on 1/13/26 5:00 pm</time></li><li>Read in 12 minutes</li></ul></section><section class="sidecar author" aria-labelledby="author-martin-st%C3%BChmer"><header><h2 id="author-martin-st%C3%BChmer"><a href="/authors/martin/" rel="author" title="Martin Stühmer" itemprop="url">Author <span itemprop="name">Martin Stühmer</span></a></h2></header><figure class="round"><picture><img src="https://www.gravatar.com/avatar/6da4a47ff9cc2b750452225fdb2582f7?d=identicon&amp;s=200" alt="Martin Stühmer" loading="lazy" decoding="async" title="Martin Stühmer"></picture></figure><section class="content" itemprop="description">Martin is a software architect and developer who has spent nearly two decades navigating the .NET ecosystem from Framework 2.0 to modern .NET 10. As Director Consulting Services at CGI and a Microsoft Certified Trainer, he specializes in cloud-native solutions, enterprise architecture, and Risk and Cost Driven Architecture (RCDA). His mission is straightforward - help teams build quality software that survives contact with production. He contributes to open-source communities through NuGet packages, writes about pragmatic software engineering on this blog, and trains developers who want substance over buzzwords.</section><a class="link" href="/authors/martin/" rel="author" aria-label="Author Martin Stühmer: Read more"></a></section><section class="sidecar social" aria-labelledby="social-heading"><header><h2 id="social-heading">Social media</h2></header><nav class="social-nav" role="navigation" aria-label="Social media links"><a class="icon bluesky" href="https://bsky.app/profile/samtrion.net" rel="noopener external" aria-label="Follow me on Bluesky" target="_blank"><span class="fa-stack fa-fw fa-1x" aria-hidden="true"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-bluesky fa-stack-1x fa-inverse"></i>
</span></a><a class="icon github" href="https://github.com/samtrion" rel="noopener external" aria-label="Follow me on GitHub" target="_blank"><i class="fa-brands fa-fw fa-github fa-2x" aria-hidden="true"></i></a>
<a class="icon linkedin" href="https://www.linkedin.com/in/martin-stuehmer" rel="noopener external" aria-label="Follow me on LinkedIn" target="_blank"><span class="fa-stack fa-fw fa-1x" aria-hidden="true"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-linkedin-in fa-stack-1x fa-inverse"></i>
</span></a><a class="icon stackoverflow" href="https://stackoverflow.com/users/10574963" rel="noopener external" aria-label="Follow me on Stack Overflow" target="_blank"><span class="fa-stack fa-fw fa-1x" aria-hidden="true"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fab fa-stack-overflow fa-stack-1x fa-inverse"></i>
</span></a><a class="icon mail" href="mailto:martin@daily-devops.net" aria-label="Contact me by e-mail" target="_blank"><span class="fa-stack fa-fw fa-1x" aria-hidden="true"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
</span></a><a class="icon feed" href="/authors/martin/feed.rss" aria-label="My RSS feed" target="_blank"><span class="fa-stack fa-fw fa-1x" aria-hidden="true"><i class="fas fa-circle fa-stack-2x"></i>
<i class="fas fa-rss fa-stack-1x fa-inverse"></i></span></a></nav></section><section class="sidecar share" aria-labelledby="share-heading"><header><h2 id="share-heading">Share this content</h2></header><a class="icon linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fdaily-devops.net%2fposts%2fkubernetes-not-platform-strategy%2f&text=Kubernetes%20Is%20Not%20a%20Platform%20Strategy%0a%20%26mdash%3b%20Daily%20DevOps%20%26%20.NET" target="_blank" rel="noopener external" aria-label="Share on LinkedIn" title="Share on LinkedIn" target="_blank"><i class="fa-brands fa-fw fa-linkedin-in fa-2x"></i></a>
<a class="icon bluesky" href="https://bsky.app/intent/compose?text=Kubernetes%20Is%20Not%20a%20Platform%20Strategy%0a%20%26mdash%3b%20Daily%20DevOps%20%26%20.NET%20https%3a%2f%2fdaily-devops.net%2fposts%2fkubernetes-not-platform-strategy%2f" target="_blank" rel="noopener external" aria-label="Share on Bluesky" title="Share on Bluesky" target="_blank"><i class="fa-brands fa-fw fa-bluesky fa-2x"></i></a></section><section class="sidecar tags" aria-labelledby="tags-heading"><header><h2 id="tags-heading">Tags</h2></header><ul class="tags" role="list"><li class="tag" role="listitem"><a href="/tags/dotnet/" hreflang="en" rel="tag" title=".NET" aria-label=".NET tag"><i class="fas fa-tag" aria-hidden="true"></i>&nbsp;.NET</a></li><li class="tag" role="listitem"><a href="/tags/architecture/" hreflang="en" rel="tag" title="Architecture" aria-label="Architecture tag"><i class="fas fa-tag" aria-hidden="true"></i>&nbsp;Architecture</a></li><li class="tag" role="listitem"><a href="/tags/cloudnative/" hreflang="en" rel="tag" title="Cloud native" aria-label="Cloud native tag"><i class="fas fa-tag" aria-hidden="true"></i>&nbsp;Cloud native</a></li><li class="tag" role="listitem"><a href="/tags/kubernetes/" hreflang="en" rel="tag" title="Kubernetes" aria-label="Kubernetes tag"><i class="fas fa-tag" aria-hidden="true"></i>&nbsp;Kubernetes</a></li><li class="tag" role="listitem"><a href="/tags/platform-engineering/" hreflang="en" rel="tag" title="Platform Engineering" aria-label="Platform Engineering tag"><i class="fas fa-tag" aria-hidden="true"></i>&nbsp;Platform Engineering</a></li></ul></section><section class="sidecar related" aria-labelledby="related-heading"><header><h2 id="related-heading">Related content</h2></header><article class="post"><figure class="square"><picture><source srcset="/images/observability-80x80.webp?v=5dd4e78ba1b8cd612574e40b272841e9" type="image/webp"><source srcset="/images/observability-80x80.png?v=53f54ab518d5253040a9c4eff2a6b8ff" type="image/png"><img src="/images/observability.png?v=48acbd668436dc9dd3287b4ec5564639" alt="Why Your Logging Strategy Fails in Production" loading="lazy" decoding="async" title="Why Your Logging Strategy Fails in Production"></picture></figure><header><h2><a href="/posts/dotnet-advanced-logging/" rel="bookmark">Why Your Logging Strategy Fails in Production</a></h2></header><section class="content" role="region"><p>Let me tell you what I&rsquo;ve learned over the years from watching teams deploy logging strategies that looked great on paper and failed spectacularly at 3 AM when production burned.</p><p>It&rsquo;s not that they didn&rsquo;t know the theory. They&rsquo;d read the Azure documentation. They&rsquo;d seen the structured logging samples. They&rsquo;d studied distributed tracing. The real problem was different: they knew <em>what</em> to do but had no idea <em>why</em> it mattered until production broke catastrophically.</p></section></article><article class="post"><figure class="square"><picture><source srcset="/images/azure-aks-networking-80x80.webp?v=53cb3cbe8e42c40b4742e0504d05f7df" type="image/webp"><source srcset="/images/azure-aks-networking-80x80.png?v=5222a7b18515bd9d3098abe7fbce0d8e" type="image/png"><img src="/images/azure-aks-networking.png?v=18bd7f57024c4453f7415c91251112a9" alt="AKS Network Policies: The Security Layer Your Cluster Is Missing" loading="lazy" decoding="async" title="AKS Network Policies: The Security Layer Your Cluster Is Missing"></picture></figure><header><h2><a href="/posts/aks-network-policies-zero-trust/" rel="bookmark">AKS Network Policies: The Security Layer Your Cluster Is Missing</a></h2></header><section class="content" role="region"><p>Network segmentation is a fundamental security control for modern Kubernetes environments. AKS supports multiple networking models such as kubenet, Azure CNI, and overlay CNIs. The networking model matters, but the decisive factor for enforcing isolation and compliance is the consistent application of network policies.</p><p>This article describes how network policies work in AKS, the available engines, practical examples, and recommended practices for enforcing a zero-trust posture within a cluster.</p></section></article><article class="post"><figure class="square"><picture><source srcset="/images/code-sharpens-thinking-80x80.webp?v=75d7ddb9d60ec33eefbcf391f1255c3f" type="image/webp"><source srcset="/images/code-sharpens-thinking-80x80.png?v=a151d2d240b300c6db90670c63c640fa" type="image/png"><img src="/images/code-sharpens-thinking.png?v=9022f5abe33b87869328198596f2ad25" alt="Why Real Professionals Will Never Be Replaced by AI
" loading="lazy" decoding="async" title="Why Real Professionals Will Never Be Replaced by AI
"></picture></figure><header><h2><a href="/posts/code-sharpens-thinking/" rel="bookmark">Why Real Professionals Will Never Be Replaced by AI</a></h2></header><section class="content" role="region">The elephant everyone ignores: AI can generate code faster than you can type. GitHub Copilot autocompletes entire functions. ChatGPT builds APIs from prompts. Typing is dead. So why will real professionals never be replaced? Because <strong>&ldquo;vibe coding&rdquo;</strong>—describe what you want, ship what AI generates—is a productivity illusion that collapses spectacularly in production. When code generation becomes trivial, understanding what that code costs, where it fails, why it breaks under load becomes everything. AI generates syntax. Professionals understand execution, failure modes, operational cost, and production consequences. The differentiator isn&rsquo;t typing speed—it&rsquo;s mastering <strong>the feedback loop</strong>: write code, watch it fail, understand why, refine thinking. This discipline can&rsquo;t be automated. Prompt engineers generate code. Real professionals ensure it survives contact with reality.</section></article></section></aside><nav class="pager" aria-label="Article navigation" role="navigation"><a class="next" href="/posts/feedback-loop-ai-cant-replace/" rel="next" aria-label="Next: The Feedback Loop That AI Can't Replace
"><span class="sub"><i class="fas fa-backward" aria-hidden="true"></i>&emsp;<span class="sr-only">Next</span></span><p class="title">The Feedback Loop That AI Can't Replace</p></a><a class="prev" href="/posts/kehrwoche-technical-debt/" rel="prev" aria-label="Previous: Kehrwoche: What Swabian Cleaning Teaches About Technical Debt"><span class="sub"><span class="sr-only">Previous</span>&emsp;<i class="fas fa-forward" aria-hidden="true"></i></span><p class="title">Kehrwoche: What Swabian Cleaning Teaches About Technical Debt</p></a></nav></main><footer class="footer"><div class="divider"></div><nav><ul class="navigation"><li><a href="/tags/">Tags</a></li><li><a href="/legal-notice/">Legal notice</a></li></ul></nav><div class="copyright">&copy; 2023 - 2026 Daily DevOps & .NET</div></footer><script defer src="/js/below.min.a6ce9164454b6077f17ae856f280442d9f3a1e21f06189f71c21f75144b22a0c8cf32bafd61d440e9d90ca02bf738b6a8ef19f5ec20285d8ca574f98c91ebaf2.js" integrity="sha512-ps6RZEVLYHfxeuhW8oBELZ86HiHwYYn3HCH3UUSyKgyM8yuv1h1EDp2QygK/c4tqjvGfXsIChdjKV0+YyR668g==" crossorigin="anonymous"></script><img class="vgwort" src="https://vg05.met.vgwort.de/na/730ad363961a4e9e8e5be06aaa0b3c09" width="1" height="1" alt="VG Wort" title="VG Wort"></body></html>